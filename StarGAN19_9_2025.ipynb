{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakry121/DCCRN/blob/master/StarGAN19_9_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kAJ4GIvxrT6",
        "outputId": "f7ef8afb-3ce0-4bdc-a794-bc0c43bbe681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3El1BOAtxvbT",
        "outputId": "0b79994a-9c44-43b2-a0d8-e2b2ce2161c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ready: (1300, 120, 128, 1) (1300, 8)\n"
          ]
        }
      ],
      "source": [
        "!pip install librosa soundfile tensorflow --quiet\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Path to SAVDESS (update if different)\n",
        "data_dir = \"/content/drive/MyDrive/STARGAN/RAVDESS \"\n",
        "\n",
        "# Emotions in SAVDESS\n",
        "emotions = {\n",
        "    \"01\": \"neutral\",\n",
        "    \"02\": \"calm\",\n",
        "    \"03\": \"happy\",\n",
        "    \"04\": \"sad\",\n",
        "    \"05\": \"angry\",\n",
        "    \"06\": \"fearful\",\n",
        "    \"07\": \"disgust\",\n",
        "    \"08\": \"surprised\"\n",
        "}\n",
        "num_classes = len(emotions)\n",
        "\n",
        "# Extract MFCCs + Delta + Delta-Delta\n",
        "def extract_mfcc(file_path, sr=16000, n_mfcc=40, max_len=128):\n",
        "    y, sr = librosa.load(file_path, sr=sr)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "    # First derivative (Δ) and second derivative (ΔΔ)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    # Stack features → shape: (3 * n_mfcc, time)\n",
        "    mfcc_combined = np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n",
        "\n",
        "    # Normalize (optional but recommended)\n",
        "    mfcc_combined = librosa.util.normalize(mfcc_combined)\n",
        "\n",
        "    # Pad/crop to fixed size\n",
        "    if mfcc_combined.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc_combined.shape[1]\n",
        "        mfcc_combined = np.pad(mfcc_combined, ((0,0),(0,pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc_combined = mfcc_combined[:, :max_len]\n",
        "\n",
        "    return mfcc_combined\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "X, y = [], []\n",
        "for actor in os.listdir(data_dir):\n",
        "    actor_path = os.path.join(data_dir, actor)\n",
        "    if os.path.isdir(actor_path):\n",
        "        for file in os.listdir(actor_path):\n",
        "            if file.endswith(\".wav\"):\n",
        "                emotion_id = file.split(\"-\")[2]  # e.g., \"03\" → happy\n",
        "                if emotion_id in emotions:\n",
        "                    mfcc_feat = extract_mfcc(os.path.join(actor_path, file))\n",
        "                    X.append(mfcc_feat)\n",
        "                    y.append(int(emotion_id)-1)\n",
        "\n",
        "X = np.array(X)[..., np.newaxis]  # (samples, 3*n_mfcc, max_len, 1)\n",
        "y = np.array(y)\n",
        "y_onehot = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "print(\"Dataset ready:\", X.shape, y_onehot.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QgrX-wx80VRl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Concatenate, ReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6eTlBU72xvkK",
        "outputId": "7b7181eb-5c93-4340-ab93-039ac4127a1b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15360\u001b[0m)     │    \u001b[38;5;34m138,240\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m2\u001b[0m)                │            │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │      \u001b[38;5;34m2,112\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │    \u001b[38;5;34m131,200\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m262,272\u001b[0m │ re_lu_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │    \u001b[38;5;34m131,136\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m577\u001b[0m │ re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15360</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">138,240</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │ re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_transpose_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> │ re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m665,537\u001b[0m (2.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665,537</span> (2.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m665,537\u001b[0m (2.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665,537</span> (2.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15360\u001b[0m)     │    \u001b[38;5;34m138,240\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m2\u001b[0m)                │            │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │      \u001b[38;5;34m2,112\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ leaky_re_lu         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │    \u001b[38;5;34m131,200\u001b[0m │ leaky_re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ leaky_re_lu_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122880\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │    \u001b[38;5;34m122,881\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15360</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">138,240</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ leaky_re_lu         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │ leaky_re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ leaky_re_lu_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122880</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">122,881</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m394,433\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,433</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m394,433\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,433</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def build_generator(input_shape=(120, 128, 1), num_classes=num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    labels = tf.keras.Input(shape=(num_classes,))\n",
        "\n",
        "    # Broadcast label to spectrogram shape\n",
        "    label_map = tf.keras.layers.Dense(np.prod(input_shape))(labels)\n",
        "    label_map = tf.keras.layers.Reshape(input_shape)(label_map)\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([inputs, label_map])\n",
        "    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    outputs = tf.keras.layers.Conv2D(1, 3, padding='same', activation='tanh')(x)\n",
        "\n",
        "    return tf.keras.Model([inputs, labels], outputs)\n",
        "\n",
        "def build_discriminator(input_shape=(120, 128, 1), num_classes=num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    labels = tf.keras.Input(shape=(num_classes,))\n",
        "\n",
        "    label_map = tf.keras.layers.Dense(np.prod(input_shape))(labels)\n",
        "    label_map = tf.keras.layers.Reshape(input_shape)(label_map)\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([inputs, label_map])\n",
        "    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
        "    x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return tf.keras.Model([inputs, labels], outputs)\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "generator.summary()\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu7IEg3QxvoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9429572a-3321-4643-c152-64452986a471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/100\n",
            "Epoch 1: D_loss=0.5784, G_loss=0.9185\n",
            "\n",
            "Epoch 2/100\n",
            "Epoch 2: D_loss=0.6473, G_loss=0.8050\n",
            "\n",
            "Epoch 3/100\n",
            "Epoch 3: D_loss=0.6289, G_loss=0.7697\n",
            "\n",
            "Epoch 4/100\n",
            "Epoch 4: D_loss=0.6541, G_loss=0.6835\n",
            "\n",
            "Epoch 5/100\n",
            "Epoch 5: D_loss=0.6540, G_loss=0.7437\n",
            "\n",
            "Epoch 6/100\n",
            "Epoch 6: D_loss=0.6517, G_loss=0.6916\n",
            "\n",
            "Epoch 7/100\n",
            "Epoch 7: D_loss=0.6649, G_loss=0.7020\n",
            "\n",
            "Epoch 8/100\n",
            "Epoch 8: D_loss=0.6228, G_loss=0.7899\n",
            "\n",
            "Epoch 9/100\n",
            "Epoch 9: D_loss=0.6081, G_loss=0.8225\n",
            "\n",
            "Epoch 10/100\n",
            "Epoch 10: D_loss=0.6043, G_loss=0.8319\n",
            "\n",
            "Epoch 11/100\n",
            "Epoch 11: D_loss=0.6024, G_loss=0.8654\n",
            "\n",
            "Epoch 12/100\n",
            "Epoch 12: D_loss=0.6359, G_loss=0.7456\n",
            "\n",
            "Epoch 13/100\n",
            "Epoch 13: D_loss=0.6511, G_loss=0.7113\n",
            "\n",
            "Epoch 14/100\n",
            "Epoch 14: D_loss=0.6803, G_loss=0.6802\n",
            "\n",
            "Epoch 15/100\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "epochs = 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y_onehot)).shuffle(100).batch(batch_size)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "g_opt = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "d_opt = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    for mel_batch, label_batch in dataset:\n",
        "        noise_labels = tf.random.uniform(shape=(mel_batch.shape[0],), minval=0, maxval=num_classes, dtype=tf.int32)\n",
        "        fake_labels = tf.one_hot(noise_labels, depth=num_classes)\n",
        "\n",
        "        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
        "            # Fake generation\n",
        "            fake_mels = generator([mel_batch, fake_labels], training=True)\n",
        "\n",
        "            real_out = discriminator([mel_batch, label_batch], training=True)\n",
        "            fake_out = discriminator([fake_mels, fake_labels], training=True)\n",
        "\n",
        "            d_loss_real = cross_entropy(tf.ones_like(real_out), real_out)\n",
        "            d_loss_fake = cross_entropy(tf.zeros_like(fake_out), fake_out)\n",
        "            d_loss = 0.5*(d_loss_real + d_loss_fake)\n",
        "\n",
        "            g_loss = cross_entropy(tf.ones_like(fake_out), fake_out)\n",
        "\n",
        "        d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "        g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
        "        d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
        "        g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: D_loss={d_loss.numpy():.4f}, G_loss={g_loss.numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngOaoWMZxvrp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick one example\n",
        "idx = np.random.randint(len(X))\n",
        "sample = X[idx:idx+1]\n",
        "orig_label = y[idx]\n",
        "\n",
        "# Target: Angry (04)\n",
        "target_label = tf.one_hot([4], depth=num_classes)\n",
        "\n",
        "# Generate fake\n",
        "fake_mel = generator([tf.constant(sample), target_label], training=False)[0].numpy().squeeze()\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(sample[0,:,:,0], aspect=\"auto\", origin=\"lower\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Fake Angry\")\n",
        "plt.imshow(fake_mel, aspect=\"auto\", origin=\"lower\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3UBeMRsatjR"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def mel_to_audio(mel, sr=16000, n_fft=512):\n",
        "    \"\"\"Convert mel-spectrogram (dB) back to waveform.\"\"\"\n",
        "    power = librosa.db_to_power(mel)  # dB → power\n",
        "    stft = librosa.feature.inverse.mel_to_stft(power, sr=sr, n_fft=n_fft)\n",
        "    audio = librosa.griffinlim(stft, n_iter=60)\n",
        "    return audio\n",
        "\n",
        "# Convert back both audios\n",
        "fake_audio = mel_to_audio(fake_mel, sr=16000)\n",
        "orig_audio = mel_to_audio(sample[0,:,:,0], sr=16000)\n",
        "\n",
        "# ====== Plot & Listen ======\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "# Original waveform\n",
        "plt.subplot(2,2,1)\n",
        "plt.title(\"Original Waveform\")\n",
        "librosa.display.waveshow(orig_audio, sr=16000)\n",
        "\n",
        "# Original mel\n",
        "plt.subplot(2,2,2)\n",
        "plt.title(\"Original Mel\")\n",
        "librosa.display.specshow(sample[0,:,:,0], sr=16000, x_axis=\"time\", y_axis=\"mel\")\n",
        "\n",
        "# Fake waveform\n",
        "plt.subplot(2,2,3)\n",
        "plt.title(\"Generated Waveform\")\n",
        "librosa.display.waveshow(fake_audio, sr=16000)\n",
        "\n",
        "# Fake mel\n",
        "plt.subplot(2,2,4)\n",
        "plt.title(\"Generated Mel (Angry)\")\n",
        "librosa.display.specshow(fake_mel, sr=16000, x_axis=\"time\", y_axis=\"mel\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Listen to both\n",
        "print(\"🔊 Original audio:\")\n",
        "display(Audio(orig_audio, rate=16000))\n",
        "\n",
        "print(\"🔊 Generated angry audio:\")\n",
        "display(Audio(fake_audio, rate=16000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gCogSaSneC6"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Convert back from dB to power\n",
        "fake_power = librosa.db_to_power(fake_mel)\n",
        "\n",
        "# Approximate STFT from mel\n",
        "fake_stft = librosa.feature.inverse.mel_to_stft(fake_power, sr=16000, n_fft=512)\n",
        "\n",
        "# Reconstruct waveform with Griffin-Lim\n",
        "fake_audio = librosa.griffinlim(fake_stft, n_iter=60)\n",
        "\n",
        "# Listen\n",
        "print(\"🔊 Playing generated audio...\")\n",
        "display(Audio(fake_audio, rate=16000))\n",
        "\n",
        "# Optionally, compare with original\n",
        "orig_power = librosa.db_to_power(sample[0,:,:,0])\n",
        "orig_stft = librosa.feature.inverse.mel_to_stft(orig_power, sr=16000, n_fft=512)\n",
        "orig_audio = librosa.griffinlim(orig_stft, n_iter=60)\n",
        "\n",
        "print(\"🔊 Playing original audio...\")\n",
        "display(Audio(orig_audio, rate=16000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC2NKfktLpbq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VwuQDboLqp7"
      },
      "outputs": [],
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dchrou9KLw_4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFz4ClZov9gZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY7tWWCbL417"
      },
      "outputs": [],
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "subO0yaAMG8H"
      },
      "outputs": [],
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHpiiNpTMREu"
      },
      "outputs": [],
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4bV2afCMSQq"
      },
      "outputs": [],
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5477615"
      },
      "source": [
        "# Task\n",
        "Modify the code to use MFCCs instead of Mel-Spectrograms for speech emotion generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfa88567"
      },
      "source": [
        "## Modify data loading\n",
        "\n",
        "### Subtask:\n",
        "Update the `extract_melspectrogram` function to extract MFCCs instead of Mel-Spectrograms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5f84959"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to modify the `extract_melspectrogram` function to extract MFCCs. This involves renaming the function, changing the librosa feature extraction call, adjusting parameters, and updating the padding/cropping logic and docstring.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21f25358"
      },
      "outputs": [],
      "source": [
        "# Extract MFCCs\n",
        "def extract_mfcc(file_path, sr=16000, n_mfcc=40, max_len=128):\n",
        "    \"\"\"Extracts MFCC features from an audio file.\"\"\"\n",
        "    y, sr = librosa.load(file_path, sr=sr)\n",
        "    # Use mfcc instead of melspectrogram\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "    # Pad/crop to fixed size\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, ((0,0),(0,pad_width)), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    return mfcc\n",
        "\n",
        "# Load dataset using the new function\n",
        "X, y = [], []\n",
        "for actor in os.listdir(data_dir):\n",
        "    actor_path = os.path.join(data_dir, actor)\n",
        "    if os.path.isdir(actor_path):\n",
        "        for file in os.listdir(actor_path):\n",
        "            if file.endswith(\".wav\"):\n",
        "                emotion_id = file.split(\"-\")[2]  # e.g., \"03\" → happy\n",
        "                if emotion_id in emotions:\n",
        "                    mfcc = extract_mfcc(os.path.join(actor_path, file))\n",
        "                    X.append(mfcc)\n",
        "                    y.append(int(emotion_id)-1)\n",
        "\n",
        "X = np.array(X)[..., np.newaxis]  # (samples, n_mfcc, max_len, 1)\n",
        "y = np.array(y)\n",
        "y_onehot = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "print(\"Dataset ready:\", X.shape, y_onehot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByyFjXFhOlLV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqEyvqSHK0E5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.signal import get_window\n",
        "\n",
        "def init_kernels(win_len, win_inc, fft_len, win_type=None, invers=False):\n",
        "    if win_type == 'None' or win_type is None:\n",
        "        window = np.ones(win_len)\n",
        "    else:\n",
        "        window = get_window(win_type, win_len, fftbins=True)#**0.5\n",
        "\n",
        "    N = fft_len\n",
        "    fourier_basis = np.fft.rfft(np.eye(N))[:win_len]\n",
        "    real_kernel = np.real(fourier_basis)\n",
        "    imag_kernel = np.imag(fourier_basis)\n",
        "    kernel = np.concatenate([real_kernel, imag_kernel], 1).T\n",
        "\n",
        "    if invers :\n",
        "        kernel = np.linalg.pinv(kernel).T\n",
        "\n",
        "    kernel = kernel*window\n",
        "    kernel = kernel[:, None, :]\n",
        "    return torch.from_numpy(kernel.astype(np.float32)), torch.from_numpy(window[None,:,None].astype(np.float32))\n",
        "\n",
        "\n",
        "class ConvSTFT(nn.Module):\n",
        "\n",
        "    def __init__(self, win_len, win_inc, fft_len=None, win_type='hamming', feature_type='real', fix=True):\n",
        "        super(ConvSTFT, self).__init__()\n",
        "\n",
        "        if fft_len == None:\n",
        "            self.fft_len = np.int(2**np.ceil(np.log2(win_len)))\n",
        "        else:\n",
        "            self.fft_len = fft_len\n",
        "\n",
        "        kernel, _ = init_kernels(win_len, win_inc, self.fft_len, win_type)\n",
        "        #self.weight = nn.Parameter(kernel, requires_grad=(not fix))\n",
        "        self.register_buffer('weight', kernel)\n",
        "        self.feature_type = feature_type\n",
        "        self.stride = win_inc\n",
        "        self.win_len = win_len\n",
        "        self.dim = self.fft_len\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 2:\n",
        "            inputs = torch.unsqueeze(inputs, 1)\n",
        "        inputs = F.pad(inputs,[self.win_len-self.stride, self.win_len-self.stride])\n",
        "        outputs = F.conv1d(inputs, self.weight, stride=self.stride)\n",
        "\n",
        "        if self.feature_type == 'complex':\n",
        "            return outputs\n",
        "        else:\n",
        "            dim = self.dim//2+1\n",
        "            real = outputs[:, :dim, :]\n",
        "            imag = outputs[:, dim:, :]\n",
        "            mags = torch.sqrt(real**2+imag**2)\n",
        "            phase = torch.atan2(imag, real)\n",
        "            return mags, phase\n",
        "\n",
        "class ConviSTFT(nn.Module):\n",
        "\n",
        "    def __init__(self, win_len, win_inc, fft_len=None, win_type='hamming', feature_type='real', fix=True):\n",
        "        super(ConviSTFT, self).__init__()\n",
        "        if fft_len == None:\n",
        "            self.fft_len = np.int(2**np.ceil(np.log2(win_len)))\n",
        "        else:\n",
        "            self.fft_len = fft_len\n",
        "        kernel, window = init_kernels(win_len, win_inc, self.fft_len, win_type, invers=True)\n",
        "        #self.weight = nn.Parameter(kernel, requires_grad=(not fix))\n",
        "        self.register_buffer('weight', kernel)\n",
        "        self.feature_type = feature_type\n",
        "        self.win_type = win_type\n",
        "        self.win_len = win_len\n",
        "        self.stride = win_inc\n",
        "        self.stride = win_inc\n",
        "        self.dim = self.fft_len\n",
        "        self.register_buffer('window', window)\n",
        "        self.register_buffer('enframe', torch.eye(win_len)[:,None,:])\n",
        "\n",
        "    def forward(self, inputs, phase=None):\n",
        "        \"\"\"\n",
        "        inputs : [B, N+2, T] (complex spec) or [B, N//2+1, T] (mags)\n",
        "        phase: [B, N//2+1, T] (if not none)\n",
        "        \"\"\"\n",
        "\n",
        "        if phase is not None:\n",
        "            real = inputs*torch.cos(phase)\n",
        "            imag = inputs*torch.sin(phase)\n",
        "            inputs = torch.cat([real, imag], 1)\n",
        "        outputs = F.conv_transpose1d(inputs, self.weight, stride=self.stride)\n",
        "\n",
        "        # this is from torch-stft: https://github.com/pseeth/torch-stft\n",
        "        t = self.window.repeat(1,1,inputs.size(-1))**2\n",
        "        coff = F.conv_transpose1d(t, self.enframe, stride=self.stride)\n",
        "        outputs = outputs/(coff+1e-8)\n",
        "        #outputs = torch.where(coff == 0, outputs, outputs/coff)\n",
        "        outputs = outputs[...,self.win_len-self.stride:-(self.win_len-self.stride)]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def test_fft():\n",
        "    torch.manual_seed(20)\n",
        "    win_len = 320\n",
        "    win_inc = 160\n",
        "    fft_len = 512\n",
        "    inputs = torch.randn([1, 1, 16000*4])\n",
        "    fft = ConvSTFT(win_len, win_inc, fft_len, win_type='hann', feature_type='real')\n",
        "    import librosa\n",
        "\n",
        "    outputs1 = fft(inputs)[0]\n",
        "    outputs1 = outputs1.numpy()[0]\n",
        "    np_inputs = inputs.numpy().reshape([-1])\n",
        "    librosa_stft = librosa.stft(np_inputs, win_length=win_len, n_fft=fft_len, hop_length=win_inc, center=False)\n",
        "    print(np.mean((outputs1 - np.abs(librosa_stft))**2))\n",
        "\n",
        "\n",
        "def test_ifft1():\n",
        "    import soundfile as sf\n",
        "    N = 400\n",
        "    inc = 100\n",
        "    fft_len=512\n",
        "    torch.manual_seed(N)\n",
        "    data = np.random.randn(16000*8)[None,None,:]\n",
        "#    data = sf.read('../ori.wav')[0]\n",
        "    inputs = data.reshape([1,1,-1])\n",
        "    fft = ConvSTFT(N, inc, fft_len=fft_len, win_type='hann', feature_type='complex')\n",
        "    ifft = ConviSTFT(N, inc, fft_len=fft_len, win_type='hann', feature_type='complex')\n",
        "    inputs = torch.from_numpy(inputs.astype(np.float32))\n",
        "    outputs1 = fft(inputs)\n",
        "    print(outputs1.shape)\n",
        "    outputs2 = ifft(outputs1)\n",
        "    sf.write('conv_stft.wav', outputs2.numpy()[0,0,:],16000)\n",
        "    print('wav MSE', torch.mean(torch.abs(inputs[...,:outputs2.size(2)]-outputs2)**2))\n",
        "\n",
        "\n",
        "def test_ifft2():\n",
        "    N = 400\n",
        "    inc = 100\n",
        "    fft_len=512\n",
        "    np.random.seed(20)\n",
        "    torch.manual_seed(20)\n",
        "    t = np.random.randn(16000*4)*0.001\n",
        "    t = np.clip(t, -1, 1)\n",
        "    #input = torch.randn([1,16000*4])\n",
        "    input = torch.from_numpy(t[None,None,:].astype(np.float32))\n",
        "\n",
        "    fft = ConvSTFT(N, inc, fft_len=fft_len, win_type='hann', feature_type='complex')\n",
        "    ifft = ConviSTFT(N, inc, fft_len=fft_len, win_type='hann', feature_type='complex')\n",
        "\n",
        "    out1 = fft(input)\n",
        "    output = ifft(out1)\n",
        "    print('random MSE', torch.mean(torch.abs(input-output)**2))\n",
        "    import soundfile as sf\n",
        "    sf.write('zero.wav', output[0,0].numpy(),16000)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #test_fft()\n",
        "    test_ifft1()\n",
        "    #test_ifft2()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PItGH-uPKc4h"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python -u\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Copyright  2018  Northwestern Polytechnical University (author: Ke Wang)\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "def show_params(nnet):\n",
        "    print(\"=\" * 40, \"Model Parameters\", \"=\" * 40)\n",
        "    num_params = 0\n",
        "    for module_name, m in nnet.named_modules():\n",
        "        if module_name == '':\n",
        "            for name, params in m.named_parameters():\n",
        "                print(name, params.size())\n",
        "                i = 1\n",
        "                for j in params.size():\n",
        "                    i = i * j\n",
        "                num_params += i\n",
        "    print('[*] Parameter Size: {}'.format(num_params))\n",
        "    print(\"=\" * 98)\n",
        "\n",
        "\n",
        "def show_model(nnet):\n",
        "    print(\"=\" * 40, \"Model Structures\", \"=\" * 40)\n",
        "    for module_name, m in nnet.named_modules():\n",
        "        if module_name == '':\n",
        "            print(m)\n",
        "    print(\"=\" * 98)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pUH9VyaKydR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6m4pFoRJkDa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import sys\n",
        "#from show import show_params, show_model # Removed import\n",
        "import torch.nn.functional as F\n",
        "#from conv_stft import ConvSTFT, ConviSTFT # Removed import\n",
        "\n",
        "#from complexnn import ComplexConv2d, ComplexConvTranspose2d, NavieComplexLSTM, complex_cat, ComplexBatchNorm # Removed import\n",
        "\n",
        "class DCCRN(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "                    self,\n",
        "                    rnn_layers=2,\n",
        "                    rnn_units=128,\n",
        "                    win_len=400,\n",
        "                    win_inc=100,\n",
        "                    fft_len=512,\n",
        "                    win_type='hann',\n",
        "                    masking_mode='E',\n",
        "                    use_clstm=False,\n",
        "                    use_cbn = False,\n",
        "                    kernel_size=5,\n",
        "                    kernel_num=[16,32,64,128,256,256]\n",
        "                ):\n",
        "        '''\n",
        "\n",
        "            rnn_layers: the number of lstm layers in the crn,\n",
        "            rnn_units: for clstm, rnn_units = real+imag\n",
        "\n",
        "        '''\n",
        "\n",
        "        super(DCCRN, self).__init__()\n",
        "\n",
        "        # for fft\n",
        "        self.win_len = win_len\n",
        "        self.win_inc = win_inc\n",
        "        self.fft_len = fft_len\n",
        "        self.win_type = win_type\n",
        "\n",
        "        input_dim = win_len\n",
        "        output_dim = win_len\n",
        "\n",
        "        self.rnn_units = rnn_units\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_layers = rnn_layers\n",
        "        self.kernel_size = kernel_size\n",
        "        #self.kernel_num = [2, 8, 16, 32, 128, 128, 128]\n",
        "        #self.kernel_num = [2, 16, 32, 64, 128, 256, 256]\n",
        "        self.kernel_num = [2]+kernel_num\n",
        "        self.masking_mode = masking_mode\n",
        "        self.use_clstm = use_clstm\n",
        "\n",
        "        #bidirectional=True\n",
        "        bidirectional=False\n",
        "        fac = 2 if bidirectional else 1\n",
        "\n",
        "\n",
        "        fix=True\n",
        "        self.fix = fix\n",
        "        self.stft = ConvSTFT(self.win_len, self.win_inc, fft_len, self.win_type, 'complex', fix=fix)\n",
        "        self.istft = ConviSTFT(self.win_len, self.win_inc, fft_len, self.win_type, 'complex', fix=fix)\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        for idx in range(len(self.kernel_num)-1):\n",
        "            self.encoder.append(\n",
        "                nn.Sequential(\n",
        "                    #nn.ConstantPad2d([0, 0, 0, 0], 0),\n",
        "                    ComplexConv2d(\n",
        "                        self.kernel_num[idx],\n",
        "                        self.kernel_num[idx+1],\n",
        "                        kernel_size=(self.kernel_size, 2),\n",
        "                        stride=(2, 1),\n",
        "                        padding=(2, 1)\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(self.kernel_num[idx+1]) if not use_cbn else ComplexBatchNorm(self.kernel_num[idx+1]),\n",
        "                    nn.PReLU()\n",
        "                )\n",
        "            )\n",
        "        hidden_dim = self.fft_len//(2**(len(self.kernel_num)))\n",
        "\n",
        "        if self.use_clstm:\n",
        "            rnns = []\n",
        "            for idx in range(rnn_layers):\n",
        "                rnns.append(\n",
        "                        NavieComplexLSTM(\n",
        "                        input_size= hidden_dim*self.kernel_num[-1] if idx == 0 else self.rnn_units,\n",
        "                        hidden_size=self.rnn_units,\n",
        "                        bidirectional=bidirectional,\n",
        "                        batch_first=False,\n",
        "                        projection_dim= hidden_dim*self.kernel_num[-1] if idx == rnn_layers-1 else None,\n",
        "                        )\n",
        "                    )\n",
        "                self.enhance = nn.Sequential(*rnns)\n",
        "        else:\n",
        "            self.enhance = nn.LSTM(\n",
        "                    input_size= hidden_dim*self.kernel_num[-1],\n",
        "                    hidden_size=self.rnn_units,\n",
        "                    num_layers=2,\n",
        "                    dropout=0.0,\n",
        "                    bidirectional=bidirectional,\n",
        "                    batch_first=False\n",
        "            )\n",
        "            self.tranform = nn.Linear(self.rnn_units * fac, hidden_dim*self.kernel_num[-1])\n",
        "\n",
        "        for idx in range(len(self.kernel_num)-1, 0, -1):\n",
        "            if idx != 1:\n",
        "                self.decoder.append(\n",
        "                    nn.Sequential(\n",
        "                        ComplexConvTranspose2d(\n",
        "                        self.kernel_num[idx]*2,\n",
        "                        self.kernel_num[idx-1],\n",
        "                        kernel_size =(self.kernel_size, 2),\n",
        "                        stride=(2, 1),\n",
        "                        padding=(2,0),\n",
        "                        output_padding=(1,0)\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(self.kernel_num[idx-1]) if not use_cbn else ComplexBatchNorm(self.kernel_num[idx-1]),\n",
        "                    #nn.ELU()\n",
        "                    nn.PReLU()\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                self.decoder.append(\n",
        "                    nn.Sequential(\n",
        "                        ComplexConvTranspose2d(\n",
        "                        self.kernel_num[idx]*2,\n",
        "                        self.kernel_num[idx-1],\n",
        "                        kernel_size =(self.kernel_size, 2),\n",
        "                        stride=(2, 1),\n",
        "                        padding=(2,0),\n",
        "                        output_padding=(1,0)\n",
        "                    ),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        show_model(self)\n",
        "        show_params(self)\n",
        "        self.flatten_parameters()\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        if isinstance(self.enhance, nn.LSTM):\n",
        "            self.enhance.flatten_parameters()\n",
        "\n",
        "    def forward(self, inputs, lens=None):\n",
        "        specs = self.stft(inputs)\n",
        "        real = specs[:,:self.fft_len//2+1]\n",
        "        imag = specs[:,self.fft_len//2+1:]\n",
        "        spec_mags = torch.sqrt(real**2+imag**2+1e-8)\n",
        "        spec_mags = spec_mags\n",
        "        spec_phase = torch.atan2(imag, real)\n",
        "        spec_phase = spec_phase\n",
        "        cspecs = torch.stack([real,imag],1)\n",
        "        cspecs = cspecs[:,:,1:]\n",
        "        '''\n",
        "        means = torch.mean(cspecs, [1,2,3], keepdim=True)\n",
        "        std = torch.std(cspecs, [1,2,3], keepdim=True )\n",
        "        normed_cspecs = (cspecs-means)/(std+1e-8)\n",
        "        out = normed_cspecs\n",
        "        '''\n",
        "\n",
        "        out = cspecs\n",
        "        encoder_out = []\n",
        "\n",
        "        for idx, layer in enumerate(self.encoder):\n",
        "            out = layer(out)\n",
        "        #    print('encoder', out.size())\n",
        "            encoder_out.append(out)\n",
        "\n",
        "        batch_size, channels, dims, lengths = out.size()\n",
        "        out = out.permute(3, 0, 1, 2)\n",
        "        if self.use_clstm:\n",
        "            r_rnn_in = out[:,:,:channels//2]\n",
        "            i_rnn_in = out[:,:,channels//2:]\n",
        "            r_rnn_in = torch.reshape(r_rnn_in, [lengths, batch_size, channels//2*dims])\n",
        "            i_rnn_in = torch.reshape(i_rnn_in, [lengths, batch_size, channels//2*dims])\n",
        "\n",
        "            r_rnn_in, i_rnn_in = self.enhance([r_rnn_in, i_rnn_in])\n",
        "\n",
        "            r_rnn_in = torch.reshape(r_rnn_in, [lengths, batch_size, channels//2, dims])\n",
        "            i_rnn_in = torch.reshape(i_rnn_in, [lengths, batch_size, channels//2, dims])\n",
        "            out = torch.cat([r_rnn_in, i_rnn_in],2)\n",
        "\n",
        "        else:\n",
        "            # to [L, B, C, D]\n",
        "            out = torch.reshape(out, [lengths, batch_size, channels*dims])\n",
        "            out, _ = self.enhance(out)\n",
        "            out = self.tranform(out)\n",
        "            out = torch.reshape(out, [lengths, batch_size, channels, dims])\n",
        "\n",
        "        out = out.permute(1, 2, 3, 0)\n",
        "\n",
        "        for idx in range(len(self.decoder)):\n",
        "            out = complex_cat([out,encoder_out[-1 - idx]],1)\n",
        "            out = self.decoder[idx](out)\n",
        "            out = out[...,1:]\n",
        "        #    print('decoder', out.size())\n",
        "        mask_real = out[:,0]\n",
        "        mask_imag = out[:,1]\n",
        "        mask_real = F.pad(mask_real, [0,0,1,0])\n",
        "        mask_imag = F.pad(mask_imag, [0,0,1,0])\n",
        "\n",
        "        if self.masking_mode == 'E' :\n",
        "            mask_mags = (mask_real**2+mask_imag**2)**0.5\n",
        "            real_phase = mask_real/(mask_mags+1e-8)\n",
        "            imag_phase = mask_imag/(mask_mags+1e-8)\n",
        "            mask_phase = torch.atan2(\n",
        "                            imag_phase,\n",
        "                            real_phase\n",
        "                        )\n",
        "\n",
        "            #mask_mags = torch.clamp_(mask_mags,0,100)\n",
        "            mask_mags = torch.tanh(mask_mags)\n",
        "            est_mags = mask_mags*spec_mags\n",
        "            est_phase = spec_phase + mask_phase\n",
        "            real = est_mags*torch.cos(est_phase)\n",
        "            imag = est_mags*torch.sin(est_phase)\n",
        "        elif self.masking_mode == 'C':\n",
        "            real,imag = real*mask_real-imag*mask_imag, real*mask_imag+imag*mask_real\n",
        "        elif self.masking_mode == 'R':\n",
        "            real, imag = real*mask_real, imag*mask_imag\n",
        "\n",
        "        out_spec = torch.cat([real, imag], 1)\n",
        "        out_wav = self.istft(out_spec)\n",
        "\n",
        "        out_wav = torch.squeeze(out_wav, 1)\n",
        "        #out_wav = torch.tanh(out_wav)\n",
        "        out_wav = torch.clamp_(out_wav,-1,1)\n",
        "        return out_spec,  out_wav\n",
        "\n",
        "    def get_params(self, weight_decay=0.0):\n",
        "            # add L2 penalty\n",
        "        weights, biases = [], []\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                biases += [param]\n",
        "            else:\n",
        "                weights += [param]\n",
        "        params = [{\n",
        "                     'params': weights,\n",
        "                     'weight_decay': weight_decay,\n",
        "                 }, {\n",
        "                     'params': biases,\n",
        "                     'weight_decay': 0.0,\n",
        "                 }]\n",
        "        return params\n",
        "\n",
        "    def loss(self, inputs, labels, loss_mode='SI-SNR'):\n",
        "\n",
        "        if loss_mode == 'MSE':\n",
        "            b, d, t = inputs.shape\n",
        "            labels[:,0,:]=0\n",
        "            labels[:,d//2,:]=0\n",
        "            return F.mse_loss(inputs, labels, reduction='mean')*d\n",
        "\n",
        "        elif loss_mode == 'SI-SNR':\n",
        "            #return -torch.mean(si_snr(inputs, labels))\n",
        "            return -(si_snr(inputs, labels))\n",
        "        elif loss_mode == 'MAE':\n",
        "            gth_spec, gth_phase = self.stft(labels)\n",
        "            b,d,t = inputs.shape\n",
        "            return torch.mean(torch.abs(inputs-gth_spec))*d\n",
        "\n",
        "def remove_dc(data):\n",
        "    mean = torch.mean(data, -1, keepdim=True)\n",
        "    data = data - mean\n",
        "    return data\n",
        "def l2_norm(s1, s2):\n",
        "    #norm = torch.sqrt(torch.sum(s1*s2, 1, keepdim=True))\n",
        "    #norm = torch.norm(s1*s2, 1, keepdim=True)\n",
        "\n",
        "    norm = torch.sum(s1*s2, -1, keepdim=True)\n",
        "    return norm\n",
        "\n",
        "def si_snr(s1, s2, eps=1e-8):\n",
        "    #s1 = remove_dc(s1)\n",
        "    #s2 = remove_dc(s2)\n",
        "    s1_s2_norm = l2_norm(s1, s2)\n",
        "    s2_s2_norm = l2_norm(s2, s2)\n",
        "    s_target =  s1_s2_norm/(s2_s2_norm+eps)*s2\n",
        "    e_nosie = s1 - s_target\n",
        "    target_norm = l2_norm(s_target, s_target)\n",
        "    noise_norm = l2_norm(e_nosie, e_nosie)\n",
        "    snr = 10*torch.log10((target_norm)/(noise_norm+eps)+eps)\n",
        "    return torch.mean(snr)\n",
        "\n",
        "def test_complex():\n",
        "    torch.manual_seed(20)\n",
        "    inputs = torch.randn(10,2,256,10)\n",
        "    conv = ComplexConv2d(2,32,(3,1),(2,1),(1,0))\n",
        "    tconv = ComplexConvTranspose2d(32,2,(3,1),(2,1),(1,0),(1,0))\n",
        "    out = conv(inputs)\n",
        "    print(out.shape)\n",
        "    out = tconv(out)\n",
        "    print(out.shape)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(10)\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    inputs = torch.randn([10,16000*4]).clamp_(-1,1)\n",
        "    labels = torch.randn([10,16000*4]).clamp_(-1,1)\n",
        "\n",
        "    '''\n",
        "    # DCCRN-E\n",
        "    net = DCCRN(rnn_units=256,masking_mode='E')\n",
        "    outputs = net(inputs)[1]\n",
        "    loss = net.loss(outputs, labels, loss_mode='SI-SNR')\n",
        "    print(loss)\n",
        "\n",
        "    # DCCRN-R\n",
        "    net = DCCRN(rnn_units=256,masking_mode='R')\n",
        "    outputs = net(inputs)[1]\n",
        "    loss = net.loss(outputs, labels, loss_mode='SI-SNR')\n",
        "    print(loss)\n",
        "\n",
        "    # DCCRN-C\n",
        "    net = DCCRN(rnn_units=256,masking_mode='C')\n",
        "    outputs = net(inputs)[1]\n",
        "    loss = net.loss(outputs, labels, loss_mode='SI-SNR')\n",
        "    print(loss)\n",
        "\n",
        "    '''\n",
        "    # DCCRN-CL\n",
        "    net = DCCRN(rnn_units=256,masking_mode='E',use_clstm=True,kernel_num=[32, 64, 128, 256, 256,256])\n",
        "    outputs = net(inputs)[1]\n",
        "    loss = net.loss(outputs, labels, loss_mode='SI-SNR')\n",
        "    print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzT_OxmIJKNX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def get_casual_padding1d():\n",
        "    pass\n",
        "\n",
        "def get_casual_padding2d():\n",
        "    pass\n",
        "\n",
        "class cPReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, complex_axis=1):\n",
        "        super(cPReLU,self).__init__()\n",
        "        self.r_prelu = nn.PReLU()\n",
        "        self.i_prelu = nn.PReLU()\n",
        "        self.complex_axis = complex_axis\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        real, imag = torch.chunk(inputs, 2,self.complex_axis)\n",
        "        real = self.r_prelu(real)\n",
        "        imag = self.i_prelu(imag)\n",
        "        return torch.cat([real,imag],self.complex_axis)\n",
        "\n",
        "class NavieComplexLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, projection_dim=None, bidirectional=False, batch_first=False):\n",
        "        super(NavieComplexLSTM, self).__init__()\n",
        "\n",
        "        self.input_dim = input_size//2\n",
        "        self.rnn_units = hidden_size//2\n",
        "        self.real_lstm = nn.LSTM(self.input_dim, self.rnn_units, num_layers=1, bidirectional=bidirectional, batch_first=False)\n",
        "        self.imag_lstm = nn.LSTM(self.input_dim, self.rnn_units, num_layers=1, bidirectional=bidirectional, batch_first=False)\n",
        "        if bidirectional:\n",
        "            bidirectional=2\n",
        "        else:\n",
        "            bidirectional=1\n",
        "        if projection_dim is not None:\n",
        "            self.projection_dim = projection_dim//2\n",
        "            self.r_trans = nn.Linear(self.rnn_units*bidirectional, self.projection_dim)\n",
        "            self.i_trans = nn.Linear(self.rnn_units*bidirectional, self.projection_dim)\n",
        "        else:\n",
        "            self.projection_dim = None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if isinstance(inputs,list):\n",
        "            real, imag = inputs\n",
        "        elif isinstance(inputs, torch.Tensor):\n",
        "            real, imag = torch.chunk(inputs,-1)\n",
        "        r2r_out = self.real_lstm(real)[0]\n",
        "        r2i_out = self.imag_lstm(real)[0]\n",
        "        i2r_out = self.real_lstm(imag)[0]\n",
        "        i2i_out = self.imag_lstm(imag)[0]\n",
        "        real_out = r2r_out - i2i_out\n",
        "        imag_out = i2r_out + r2i_out\n",
        "        if self.projection_dim is not None:\n",
        "            real_out = self.r_trans(real_out)\n",
        "            imag_out = self.i_trans(imag_out)\n",
        "        #print(real_out.shape,imag_out.shape)\n",
        "        return [real_out, imag_out]\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        self.imag_lstm.flatten_parameters()\n",
        "        self.real_lstm.flatten_parameters()\n",
        "\n",
        "def complex_cat(inputs, axis):\n",
        "\n",
        "    real, imag = [],[]\n",
        "    for idx, data in enumerate(inputs):\n",
        "        r, i = torch.chunk(data,2,axis)\n",
        "        real.append(r)\n",
        "        imag.append(i)\n",
        "    real = torch.cat(real,axis)\n",
        "    imag = torch.cat(imag,axis)\n",
        "    outputs = torch.cat([real, imag],axis)\n",
        "    return outputs\n",
        "\n",
        "class ComplexConv2d(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "                    self,\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=(1,1),\n",
        "                    stride=(1,1),\n",
        "                    padding=(0,0),\n",
        "                    dilation=1,\n",
        "                    groups = 1,\n",
        "                    causal=True,\n",
        "                    complex_axis=1,\n",
        "                ):\n",
        "        '''\n",
        "            in_channels: real+imag\n",
        "            out_channels: real+imag\n",
        "            kernel_size : input [B,C,D,T] kernel size in [D,T]\n",
        "            padding : input [B,C,D,T] padding in [D,T]\n",
        "            causal: if causal, will padding time dimension's left side,\n",
        "                    otherwise both\n",
        "\n",
        "        '''\n",
        "        super(ComplexConv2d, self).__init__()\n",
        "        self.in_channels = in_channels//2\n",
        "        self.out_channels = out_channels//2\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.causal = causal\n",
        "        self.groups = groups\n",
        "        self.dilation = dilation\n",
        "        self.complex_axis=complex_axis\n",
        "        self.real_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size, self.stride,padding=[self.padding[0],0],dilation=self.dilation, groups=self.groups)\n",
        "        self.imag_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size, self.stride,padding=[self.padding[0],0],dilation=self.dilation, groups=self.groups)\n",
        "\n",
        "        nn.init.normal_(self.real_conv.weight.data,std=0.05)\n",
        "        nn.init.normal_(self.imag_conv.weight.data,std=0.05)\n",
        "        nn.init.constant_(self.real_conv.bias,0.)\n",
        "        nn.init.constant_(self.imag_conv.bias,0.)\n",
        "\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        if self.padding[1] != 0 and self.causal:\n",
        "            inputs = F.pad(inputs,[self.padding[1], 0,0,0])\n",
        "        else:\n",
        "            inputs = F.pad(inputs,[self.padding[1], self.padding[1],0,0])\n",
        "\n",
        "        if self.complex_axis == 0:\n",
        "            real = self.real_conv(inputs)\n",
        "            imag = self.imag_conv(inputs)\n",
        "            real2real,imag2real = torch.chunk(real,2, self.complex_axis)\n",
        "            real2imag,imag2imag = torch.chunk(imag,2, self.complex_axis)\n",
        "\n",
        "        else:\n",
        "            if isinstance(inputs, torch.Tensor):\n",
        "                real,imag = torch.chunk(inputs, 2, self.complex_axis)\n",
        "\n",
        "            real2real = self.real_conv(real,)\n",
        "            imag2imag = self.imag_conv(imag,)\n",
        "\n",
        "            real2imag = self.imag_conv(real)\n",
        "            imag2real = self.real_conv(imag)\n",
        "\n",
        "        real = real2real - imag2imag\n",
        "        imag = real2imag + imag2real\n",
        "        out = torch.cat([real, imag], self.complex_axis)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ComplexConvTranspose2d(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "                    self,\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=(1,1),\n",
        "                    stride=(1,1),\n",
        "                    padding=(0,0),\n",
        "                    output_padding=(0,0),\n",
        "                    causal=False,\n",
        "                    complex_axis=1,\n",
        "                    groups=1\n",
        "                ):\n",
        "        '''\n",
        "            in_channels: real+imag\n",
        "            out_channels: real+imag\n",
        "        '''\n",
        "        super(ComplexConvTranspose2d, self).__init__()\n",
        "        self.in_channels = in_channels//2\n",
        "        self.out_channels = out_channels//2\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.output_padding=output_padding\n",
        "        self.groups = groups\n",
        "\n",
        "        self.real_conv = nn.ConvTranspose2d(self.in_channels, self.out_channels,kernel_size, self.stride,padding=self.padding,output_padding=output_padding, groups=self.groups)\n",
        "        self.imag_conv = nn.ConvTranspose2d(self.in_channels, self.out_channels,kernel_size, self.stride,padding=self.padding,output_padding=output_padding, groups=self.groups)\n",
        "        self.complex_axis=complex_axis\n",
        "\n",
        "        nn.init.normal_(self.real_conv.weight,std=0.05)\n",
        "        nn.init.normal_(self.imag_conv.weight,std=0.05)\n",
        "        nn.init.constant_(self.real_conv.bias,0.)\n",
        "        nn.init.constant_(self.imag_conv.bias,0.)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "\n",
        "        if isinstance(inputs, torch.Tensor):\n",
        "            real,imag = torch.chunk(inputs, 2, self.complex_axis)\n",
        "        elif isinstance(inputs, tuple) or isinstance(inputs, list):\n",
        "            real = inputs[0]\n",
        "            imag = inputs[1]\n",
        "        if self.complex_axis == 0:\n",
        "            real = self.real_conv(inputs)\n",
        "            imag = self.imag_conv(inputs)\n",
        "            real2real,imag2real = torch.chunk(real,2, self.complex_axis)\n",
        "            real2imag,imag2imag = torch.chunk(imag,2, self.complex_axis)\n",
        "\n",
        "        else:\n",
        "            if isinstance(inputs, torch.Tensor):\n",
        "                real,imag = torch.chunk(inputs, 2, self.complex_axis)\n",
        "\n",
        "            real2real = self.real_conv(real,)\n",
        "            imag2imag = self.imag_conv(imag,)\n",
        "\n",
        "            real2imag = self.imag_conv(real)\n",
        "            imag2real = self.real_conv(imag)\n",
        "\n",
        "        real = real2real - imag2imag\n",
        "        imag = real2imag + imag2real\n",
        "        out = torch.cat([real, imag], self.complex_axis)\n",
        "\n",
        "        return out\n",
        "# Source: https://github.com/ChihebTrabelsi/deep_complex_networks/tree/pytorch\n",
        "# from https://github.com/IMLHF/SE_DCUNet/blob/f28bf1661121c8901ad38149ea827693f1830715/models/layers/complexnn.py#L55\n",
        "\n",
        "class ComplexBatchNorm(torch.nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
        "            track_running_stats=True, complex_axis=1):\n",
        "        super(ComplexBatchNorm, self).__init__()\n",
        "        self.num_features        = num_features//2\n",
        "        self.eps                 = eps\n",
        "        self.momentum            = momentum\n",
        "        self.affine              = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "\n",
        "        self.complex_axis = complex_axis\n",
        "\n",
        "        if self.affine:\n",
        "            self.Wrr = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
        "            self.Wri = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
        "            self.Wii = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
        "            self.Br  = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
        "            self.Bi  = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
        "        else:\n",
        "            self.register_parameter('Wrr', None)\n",
        "            self.register_parameter('Wri', None)\n",
        "            self.register_parameter('Wii', None)\n",
        "            self.register_parameter('Br',  None)\n",
        "            self.register_parameter('Bi',  None)\n",
        "\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('RMr',  torch.zeros(self.num_features))\n",
        "            self.register_buffer('RMi',  torch.zeros(self.num_features))\n",
        "            self.register_buffer('RVrr', torch.ones (self.num_features))\n",
        "            self.register_buffer('RVri', torch.zeros(self.num_features))\n",
        "            self.register_buffer('RVii', torch.ones (self.num_features))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "        else:\n",
        "            self.register_parameter('RMr',                 None)\n",
        "            self.register_parameter('RMi',                 None)\n",
        "            self.register_parameter('RVrr',                None)\n",
        "            self.register_parameter('RVri',                None)\n",
        "            self.register_parameter('RVii',                None)\n",
        "            self.register_parameter('num_batches_tracked', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.RMr.zero_()\n",
        "            self.RMi.zero_()\n",
        "            self.RVrr.fill_(1)\n",
        "            self.RVri.zero_()\n",
        "            self.RVii.fill_(1)\n",
        "            self.num_batches_tracked.zero_()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            self.Br.data.zero_()\n",
        "            self.Bi.data.zero_()\n",
        "            self.Wrr.data.fill_(1)\n",
        "            self.Wri.data.uniform_(-.9, +.9) # W will be positive-definite\n",
        "            self.Wii.data.fill_(1)\n",
        "\n",
        "    def _check_input_dim(self, xr, xi):\n",
        "        assert(xr.shape == xi.shape)\n",
        "        assert(xr.size(1) == self.num_features)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #self._check_input_dim(xr, xi)\n",
        "\n",
        "        xr, xi = torch.chunk(inputs,2, axis=self.complex_axis)\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            self.num_batches_tracked += 1\n",
        "            if self.momentum is None:  # use cumulative moving average\n",
        "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
        "            else:  # use exponential moving average\n",
        "                exponential_average_factor = self.momentum\n",
        "\n",
        "        #\n",
        "        # NOTE: The precise meaning of the \"training flag\" is:\n",
        "        #       True:  Normalize using batch   statistics, update running statistics\n",
        "        #              if they are being collected.\n",
        "        #       False: Normalize using running statistics, ignore batch   statistics.\n",
        "        #\n",
        "        training = self.training or not self.track_running_stats\n",
        "        redux = [i for i in reversed(range(xr.dim())) if i!=1]\n",
        "        vdim  = [1] * xr.dim()\n",
        "        vdim[1] = xr.size(1)\n",
        "\n",
        "        #\n",
        "        # Mean M Computation and Centering\n",
        "        #\n",
        "        # Includes running mean update if training and running.\n",
        "        #\n",
        "        if training:\n",
        "            Mr, Mi = xr, xi\n",
        "            for d in redux:\n",
        "                Mr = Mr.mean(d, keepdim=True)\n",
        "                Mi = Mi.mean(d, keepdim=True)\n",
        "            if self.track_running_stats:\n",
        "                self.RMr.lerp_(Mr.squeeze(), exponential_average_factor)\n",
        "                self.RMi.lerp_(Mi.squeeze(), exponential_average_factor)\n",
        "        else:\n",
        "            Mr = self.RMr.view(vdim)\n",
        "            Mi = self.RMi.view(vdim)\n",
        "        xr, xi = xr-Mr, xi-Mi\n",
        "\n",
        "        #\n",
        "        # Variance Matrix V Computation\n",
        "        #\n",
        "        # Includes epsilon numerical stabilizer/Tikhonov regularizer.\n",
        "        # Includes running variance update if training and running.\n",
        "        #\n",
        "        if training:\n",
        "            Vrr = xr * xr\n",
        "            Vri = xr * xi\n",
        "            Vii = xi * xi\n",
        "            for d in redux:\n",
        "                Vrr = Vrr.mean(d, keepdim=True)\n",
        "                Vri = Vri.mean(d, keepdim=True)\n",
        "                Vii = Vii.mean(d, keepdim=True)\n",
        "            if self.track_running_stats:\n",
        "                self.RVrr.lerp_(Vrr.squeeze(), exponential_average_factor)\n",
        "                self.RVri.lerp_(Vri.squeeze(), exponential_average_factor)\n",
        "                self.RVii.lerp_(Vii.squeeze(), exponential_average_factor)\n",
        "        else:\n",
        "            Vrr = self.RVrr.view(vdim)\n",
        "            Vri = self.RVri.view(vdim)\n",
        "            Vii = self.RVii.view(vdim)\n",
        "        Vrr   = Vrr + self.eps\n",
        "        Vri   = Vri\n",
        "        Vii   = Vii + self.eps\n",
        "\n",
        "        #\n",
        "        # Matrix Inverse Square Root U = V^-0.5\n",
        "        #\n",
        "        # sqrt of a 2x2 matrix,\n",
        "        # - https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n",
        "        tau   = Vrr + Vii\n",
        "        delta = torch.addcmul(Vrr * Vii, -1, Vri, Vri)\n",
        "        s     = delta.sqrt()\n",
        "        t     = (tau + 2*s).sqrt()\n",
        "\n",
        "        # matrix inverse, http://mathworld.wolfram.com/MatrixInverse.html\n",
        "        rst   = (s * t).reciprocal()\n",
        "        Urr   = (s + Vii) * rst\n",
        "        Uii   = (s + Vrr) * rst\n",
        "        Uri   = (  - Vri) * rst\n",
        "\n",
        "        #\n",
        "        # Optionally left-multiply U by affine weights W to produce combined\n",
        "        # weights Z, left-multiply the inputs by Z, then optionally bias them.\n",
        "        #\n",
        "        # y = Zx + B\n",
        "        # y = WUx + B\n",
        "        # y = [Wrr Wri][Urr Uri] [xr] + [Br]\n",
        "        #     [Wir Wii][Uir Uii] [xi]   [Bi]\n",
        "        #\n",
        "        if self.affine:\n",
        "            Wrr, Wri, Wii = self.Wrr.view(vdim), self.Wri.view(vdim), self.Wii.view(vdim)\n",
        "            Zrr = (Wrr * Urr) + (Wri * Uri)\n",
        "            Zri = (Wrr * Uri) + (Wri * Uii)\n",
        "            Zir = (Wri * Urr) + (Wii * Uri)\n",
        "            Zii = (Wri * Uri) + (Wii * Uii)\n",
        "        else:\n",
        "            Zrr, Zri, Zir, Zii = Urr, Uri, Uri, Uii\n",
        "\n",
        "        yr = (Zrr * xr) + (Zri * xi)\n",
        "        yi = (Zir * xr) + (Zii * xi)\n",
        "\n",
        "        if self.affine:\n",
        "            yr = yr + self.Br.view(vdim)\n",
        "            yi = yi + self.Bi.view(vdim)\n",
        "\n",
        "        outputs = torch.cat([yr, yi], self.complex_axis)\n",
        "        return outputs\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
        "                'track_running_stats={track_running_stats}'.format(**self.__dict__)\n",
        "\n",
        "def complex_cat(inputs, axis):\n",
        "\n",
        "    real, imag = [],[]\n",
        "    for idx, data in enumerate(inputs):\n",
        "        r, i = torch.chunk(data,2,axis)\n",
        "        real.append(r)\n",
        "        imag.append(i)\n",
        "    real = torch.cat(real,axis)\n",
        "    imag = torch.cat(imag,axis)\n",
        "    outputs = torch.cat([real, imag],axis)\n",
        "    return outputs\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #import dc_crn # Removed import\n",
        "    torch.manual_seed(20)\n",
        "    onet1 = ComplexConv2d(12,12,kernel_size=(3,2),padding=(2,1)) # Changed from dc_crn.ComplexConv2d\n",
        "    onet2 = ComplexConvTranspose2d(12,12,kernel_size=(3,2),padding=(2,1)) # Changed from dc_crn.ComplexConvTranspose2d\n",
        "    inputs = torch.randn([1,12,12,10])\n",
        "#    print(onet1.real_kernel[0,0,0,0])\n",
        "    nnet1 = ComplexConv2d(12,12,kernel_size=(3,2),padding=(2,1),causal=True)\n",
        "#    print(nnet1.real_conv.weight[0,0,0,0])\n",
        "    nnet2 = ComplexConvTranspose2d(12,12,kernel_size=(3,2),padding=(2,1))\n",
        "    print(torch.mean(nnet1(inputs)-onet1(inputs)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYNCFLvLxxf4JgljkRLXta",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}